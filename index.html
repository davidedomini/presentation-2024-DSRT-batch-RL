<!DOCTYPE html><html lang="en"><head>
	<meta name="generator" content="Hugo 0.134.3">
    <meta charset="utf-8">
<title>A Reusable Simulation Pipeline for Many-Agent Reinforcement Learning</title>
<meta name="description" content="Presentation DSRT 2024 - Main Track">
<meta name="author" content="Davide Domini">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><link rel="stylesheet" href="/presentation-2024-DSRT-batch-RL/reveal-js/dist/reset.css">
<link rel="stylesheet" href="/presentation-2024-DSRT-batch-RL/reveal-js/dist/reveal.css">
  <link rel="stylesheet" href="/presentation-2024-DSRT-batch-RL/css/custom-theme.min.d3b9cb38dd4a85cc874a4dddd1f2f3df5bcdace2d0d4187f5f34497ada6b143c.css" id="theme"><link rel="stylesheet" href="/presentation-2024-DSRT-batch-RL/highlight-js/solarized-dark.min.css">
<link href="https://fonts.googleapis.com/css?family=Roboto Mono" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Oxygen Mono" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu Mono" rel="stylesheet">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
<link href="https://cdn.jsdelivr.net/gh/DanySK/css-blur-animation/blur.css" rel="stylesheet" crossorigin="anonymous">

<script src="https://kit.fontawesome.com/81ac037be0.js" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://unpkg.com/qr-code-styling@1.5.0/lib/qr-code-styling.js"></script>

  </head>
  <body>
    
    <div class="reveal">
      <div class="slides">
  

    <section><h1 id="a-reusable-simulation-pipeline-for">A Reusable Simulation Pipeline for</h1>
<h1 id="many-agent-reinforcement-learning">Many-Agent Reinforcement Learning</h1>
<p><a href="mailto:davide.domini@unibo.it"><span style="color: #BD4089">Davide Domini</span></a>,
<a href="mailto:gianluca.aguzzi@unibo.it">Gianluca Aguzzi</a> ,
<a href="mailto:danilo.pianini@unibo.it">Danilo Pianini</a>,
<a href="mailto:mirko.viroli@unibo.it">Mirko Viroli</a></p>
<p>International Symposium on Distributed Simulation and Real Time Applications @ DSRT 2024</p>
<br>
<div style="text-align: center; width: 100%;">
<img src="disi.svg" style="width: 40%">
</div>
</section><section>
<h1 id="many-agent-reinforcement-learning-1">Many-Agent Reinforcement Learning</h1>
<img src="marl.png" style="width: 90%">
</section><section>
<h1 id="marl-examples">MARL Examples</h1>

<div class="container w-100 p-0">
    <div class="row "><div class="col "><div style="text-align: center; width: 100%;">
<img src="swarm1.jpg" style="width: 100%">
</div>
</div>
<!-- 

<div class='col '><div style="text-align: center; width: 100%;">
<img src="swarm2.jpg" style="width: 100%" />
</div>
</div> -->
<div class="col "><div style="text-align: center; width: 100%;">
<img src="iov.png" style="width: 100%">
</div>
</div>
</div>
</div>
<!-- <div style="text-align: center; width: 60%;"> -->
<img src="swarm2.jpg" style="width:50%">
<!-- </div> -->
<!-- 
---
# MARL Formalization

<div style="text-align:left;">



<span class='fragment ' >
  <ul>
<li><em>SwarMDP</em> is an extension of the classical <em>Markov Decision Process</em> (MDP) model, suitable for Many-Agent RL</li>
</ul>

</span>



<span class='fragment ' >
  <ul>
<li>Formally, a SwarMDP is a tuple of a <em>swarming agent $\mathbb{A}$</em> and an <em>environment $\mathcal{E}$</em></li>
</ul>

</span>



<span class='fragment ' >
  <ul>
<li>The agent $\mathbb{A}$ is defined by:
<ul>
<li>$\mathcal{S}$: the set of possible <em>internal states</em> an agent can perceive</li>
<li>$\mathcal{O}$: the <em>environment observations</em> an agent can make</li>
<li>$\mathcal{A}$: the set of <em>actions</em> an agent can perform</li>
<li>$\mathcal{R}: \mathcal{O} \rightarrow \mathbb{R}$: a function mapping each <em>state</em> to a numerical <em>reward value</em>, reflecting the <em>desirability</em> of that state</li>
<li>$\pi: \mathcal{O} \rightarrow \mathcal{A}$: the agent&rsquo;s <em>policy</em>, mapping <em>observations to actions</em>, which can be deterministic or probabilistic</li>
</ul>
</li>
</ul>

</span>



<span class='fragment ' >
  <ul>
<li>The environment $\mathcal{E}$ represents the world in which the agent operates, and is defined by:
<ul>
<li>$\mathcal{P}$ is the <em>number of agents</em> in the system</li>
<li>$\mathbb{A}$ is a prototypical agent <em>defining properties</em> common to all agents</li>
<li>$\mathcal{T}: \mathcal{S^\mathcal{P}} \times \mathcal{A}^\mathcal{P} \times \mathcal{S^\mathcal{P}} \rightarrow \mathbb{R}^\mathcal{P}$ is the <em>global transition function</em>, determining the evolution of the system&rsquo;s state based on the current states and actions of all agents</li>
<li>$\xi: \mathcal{S^\mathcal{P}} \rightarrow \mathcal{O}^\mathcal{P}$ is a function <em>mapping the state of all agents to the corresponding observations</em> available to each agent</li>
</ul>
</li>
</ul>

</span>


</div> -->
<!-- ---

# MARL Formalization (2)

<div style="text-align:left;">



<span class='fragment ' >
  <ul>
<li>The <em>dynamic behavior</em> of the ManyRL system is captured by the following equations:
<ul>
<li>$\textbf{o}_t = \xi(\textbf{s}_t)$: agents perceive the environment as a <em>set of observations</em>, provided by the function $\mathcal{E}$, and starting from environmental state $\textbf{s}_t$</li>
<li>$\textbf{a}_t = \pi(\textbf{o}_t)$: agents choose actions according to their <em>policy</em> $\pi$</li>
<li>$(\textbf{s}_{t+1}, \textbf{r}_t) = \mathcal{T}(\textbf{s}_t, \textbf{a}_t)$: the environment <em>evolves to a new state</em>, according to the <em>transition function</em> $\mathcal{T}$</li>
</ul>
</li>
</ul>

</span>



<span class='fragment ' >
  <ul>
<li>The goal of a ManyRL system is to <em>learn a policy $\pi$</em> that maximizes the <em>expected cumulative collective reward</em> over time:
<ul>
<li>$\max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \sum_{i=1}^{\mathcal{P}} r^i_t \right]$</li>
</ul>
</li>
</ul>

</span>



<span class='fragment ' >
  <ul>
<li>The <em>most common approach</em> to optimize the cumulative collective reward in ManyRL systems:
<ul>
<li><em>Deep Reinforcement Learning</em> algorithms</li>
<li><em>Centralized Training Decentralized Execution</em> (CTDE) paradigm as learning strategy</li>
</ul>
</li>
</ul>

</span>


</div> -->
</section><section>
<h1 id="motivation">Motivation</h1>
<div style="text-align:left;">


<span class="fragment ">
  <ul>
<li><strong>Challenges in Multi-Agent Reinforcement Learning (ManyRL):</strong>
<ul>
<li><em>Decentralized</em> decision-making processes lead to <em>complex system dynamics</em></li>
<li><em>Emergent behaviors</em> arise from <em>interactions</em> among numerous autonomous agents</li>
</ul>
</li>
</ul>

</span>



<span class="fragment ">
  <ul>
<li><strong>Limitations of Real-World Learning:</strong>
<ul>
<li><em>Impractical</em> due to high costs, time constraints, and legal/physical restrictions</li>
</ul>
</li>
</ul>

</span>



<span class="fragment ">
  <ul>
<li><strong>Advantages of Simulation in ManyRL Research:</strong>
<ul>
<li>Enables <em>scalable training</em> of ManyRL algorithms in a controlled, safe, and cost-effective environment</li>
<li>Facilitates <em>exploration</em> of diverse scenarios, fine-tuning of policies, and evaluation of system robustness before real-world deployment</li>
</ul>
</li>
</ul>

</span>



<span class="fragment ">
  <ul>
<li><strong>Existing Simulators:</strong>
<ul>
<li>Platforms like PettingZoo and Gazebo are designed for <em>small-scale scenarios</em></li>
<li>They lack <em>scalability</em> and <em>configurability</em>, which are critical for addressing ManyRL challenges</li>
</ul>
</li>
</ul>

</span>

</div>
</section><section>
<h1 id="comparison-with-exisisting-solutions">Comparison with exisisting solutions</h1>
<div style="font-size:0.8em">
<table>
    <thead>
        <tr>
            <th> </th>
            <th>Simulator</th>
            <th>Configurability</th>
            <th>Lifecycle Management</th>
            <th>Experience Extraction</th>
            <th>Scalability</th>
            <th>Distributed Execution</th>
            <th>Deep Learning Integration</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="4" style="writing-mode: vertical-lr; transform: rotate(180deg); text-align: center; vertical-align: middle;"><strong> General Purpose </strong></td>
            <td>NetLogo</td>
            <td style="color: green;">✔</td>
            <td><span style="color: #a63a50;"> <strong> / </strong> </span></td>
            <td><span style="color: #a63a50;"> <strong> / </strong> </span></td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
        </tr>
        <tr>
            <td>MESA</td>
            <td> <span style="color: green;">✔</span> <span style="font-size:0.7em;">(Modular) </span> </td>
            <td><span style="color: #a63a50;"> <strong> / </strong> </span></td>
            <td><span style="color: #a63a50;"> <strong> / </strong> </span></td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
        </tr>
        <tr>
            <td>Sibilla</td>
            <td> <span style="color: green;">✔</span>  <span style="font-size:0.7em;">(Multiple Specs)</span></td>
            <td style="color: green;">✔</td>
            <td><span style="color: #a63a50;"> <strong> / </strong> </span></td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
        </tr>
        <tr>
            <td class="fragment custom back" data-fragment-index="0">Alchemist</td>
            <td class="fragment custom back" data-fragment-index="0" style="color: green;">✔</td>
            <td class="fragment custom back" data-fragment-index="0" style="color: green;">✔</td>
            <td class="fragment custom back" data-fragment-index="0" style="color: green;">✔</td>
            <td class="fragment custom back" data-fragment-index="0" style="color: green;">✔</td>
            <td class="fragment custom back" data-fragment-index="0" style="color: green;">✔</td>
            <td class="fragment custom back" data-fragment-index="0"> <span style="color: #f57a00;"> <strong> ~ </strong> </span>  <span style="font-size:0.7em;">(With ScalaPy &amp; GraalPy)</span></td>
        </tr>
        <!-- <tr>
            <td colspan="7"><strong>Swarm Robotics</strong></td>
        </tr> -->
        <tr>
            <td rowspan="3" style="writing-mode: vertical-lr; transform: rotate(180deg); text-align: center;"><strong>Swarm Robotics</strong></td>
            <td>ARGoS</td>
            <td> <span style="color: green;"> ✔ </span>  <span style="font-size:0.7em;">(Limited for MARL)</span></td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
        </tr>
        <tr>
            <td>Gazebo</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
        </tr>
        <tr>
            <td>Kilombo</td>
            <td><span style="color: red;">✗</span></td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
        </tr>
        <!-- <tr>
            <td colspan="7"><strong>Multi-agent reinforcement learning</strong></td>
        </tr> -->
        <tr>
            <td rowspan="3" style="writing-mode: vertical-lr; transform: rotate(180deg); text-align: center; vertical-align: middle;"><strong>MARL</strong></td>
            <td>PettingZoo</td>
            <td> <span style="color: green;"> ✔ </span>  <span style="font-size:0.7em;">(Via Environments)</span></td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
            <td style="color: green;">✔</td>
        </tr>
        <tr>
            <td>Unity ML-Agents</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
            <td><span style="color: red;">✗</span></td>
            <td style="color: green;">✔</td>
        </tr>
        <tr>
            <td>Neural MMO</td>
            <td> <span style="color: green;"> ✔ </span> <span style="font-size:0.7em;"> (Task-Specific)</span></td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td style="color: green;">✔</td>
            <td><span style="color: red;">✗</span></td>
            <td style="color: green;">✔</td>
        </tr>
    </tbody>
</table>
</div>
<div style="font-size:0.8em; text-align:left; margin-left:3%;">
<p><span style="color: green; margin-right: 1%;"> ✔ </span> Yes</p>
<p><span style="color: #f57a00; margin-right: 1%;"> <strong>~</strong> </span> Partially, using third-party tools</p>
<p><span style="color: #a63a50; margin-right: 1%;"> <strong>/</strong> </span> Limited</p>
<p><span style="color: red; margin-right: 1%;">✗</span> No</p>
</div>
<br>
</section><section>
<h1 id="pipeline-architecture">Pipeline Architecture</h1>
<p><br> <br></p>

<div class="container w-100 p-0">
    <div class="row "><div class="col "><ul>
<li>Current observation $\rho$</li>
<li>Action computation $\gamma$</li>
<li>Environment interaction $\theta$</li>
<li>Next observation $\rho_\mathcal{+}$</li>
<li>Collective reward computation $R$</li>
<li>Experience storage $\mathbb{E}$</li>
</ul>
</div>
<div class="col "><div style="text-align: center; width: 100%;">
<img src="pipeline.svg" style="width: 100%">
</div>
</div>
</div>
</div>
</section><section>
<h1 id="marlalchemy-prototype">MARLAlchemy Prototype</h1>
<img src="prototype.svg" style="width: 85%">
<br>
</section><section>
<h1 id="experimental-evaluation-scenario">Experimental Evaluation: Scenario</h1>
<ul>
<li>Experiment on <em>multi-agent flocking behavior</em> : agents must learn to move while maintaining <em>cohesive</em> groups and avoiding <em>collisions</em></li>
<li>Cohesion among agents is defined by two hyperparameters $\delta_U$ and $\delta_L$ (<em>target distance range</em> an agent aims to maintain from its <em>neighbors</em>)</li>
</ul>
<!-- - *$100$ agents* in a Euclidean 2D space with no boundaries -->
<!-- - Each agent has *$8$ possible movement actions* corresponding to the directions on a grid (horizontal, vertical, and diagonal) -->
<ul>
<li>The observation space for each agent is defined as the <em>relative distance vector to its neighbors</em>: $\mathcal{O} =$ { $( x_i - x_j, y_i - y_j ) \mid j \in \mathcal{N}_i$ }</li>
<li>Each agent is rewarded if the maximum distance $d$ to its neighbors is within a range $]\delta_L, \delta_U [$, and it is penalized otherwise: $\mathcal{R} =  0 \text{ if } \delta_U &lt; d &lt; \delta_L, \text{ otherwise } -1$</li>
</ul>

<div class="container w-100 p-0">
    <div class="row "><div class="col "><img src="flock.gif">
</div>
<div class="col "><img src="flock2.gif">
</div>
</div>
</div>
</section><section>
<h1 id="experimental-evaluation-setup">Experimental Evaluation: Setup</h1>
<ul>
<li>Training algorithm: <em>Conservative Q-Learning</em></li>
<li><em>$9$</em> global <em>training rounds</em> followed by <em>$1$ evaluation round</em></li>
<li>Each global round consisted of one or more simulations, depending on the level of parallelism $p$ ( $p \in$ { $1,2,4,8$ } )</li>
<li>Each simulation consisted of <em>$200$ episodes</em></li>
<li>Quality metrics:
<ul>
<li>Average distance of the agents from their neighbors</li>
<li>Value of the reward function $\mathcal{R}$</li>
</ul>
</li>
<li>All the experiments are <em>publicly available</em> and <em>reproducible</em></li>
</ul>
<img src="qr.svg" width="15%" style="position: absolute; bottom: -40%; right: 0%;">
</section><section>
<h1 id="results">Results</h1>
<div style="text-align: center; width: 100%;">
<img src="results.svg" style="width: 100%">
</div>
</section><section>
<h1 id="visualizing-a-simulation">Visualizing a Simulation</h1>
<div style="text-align: center; width: 100%;">
<img src="simulation.svg" style="width: 100%">
</div>
</section><section>
<h1 id="whats-next">What’s next?</h1>
<ul>
<li>Integration of <em>additional learning algorithms</em></li>
<li>Evaluate the pipeline on a <em>wider range of many-agent learning scenarios</em></li>
<li>Integrate the pipeline within the <em>main Alchemist distribution</em></li>
</ul>
</section>

  


</div>
      

    </div>
<script type="text/javascript" src="/presentation-2024-DSRT-batch-RL/reveal-hugo/object-assign.js"></script>

<a href="/presentation-2024-DSRT-batch-RL/reveal-js/dist/print/" id="print-location" style="display: none;"></a>

<script type="application/json" id="reveal-hugo-site-params">{"custom_theme":"custom-theme.scss","custom_theme_compile":true,"custom_theme_options":{"enablesourcemap":true,"targetpath":"css/custom-theme.css"},"height":"1080","highlight_theme":"solarized-dark","history":true,"mermaid":[{}],"slide_number":true,"theme":"league","transition":"slide","transition_speed":"fast","width":"1920"}</script>
<script type="application/json" id="reveal-hugo-page-params">null</script>

<script src="/presentation-2024-DSRT-batch-RL/reveal-js/dist/reveal.js"></script>


  
  
  <script type="text/javascript" src="/presentation-2024-DSRT-batch-RL/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/presentation-2024-DSRT-batch-RL/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/presentation-2024-DSRT-batch-RL/reveal-js/plugin/zoom/zoom.js"></script>
  
  <script type="text/javascript" src="/presentation-2024-DSRT-batch-RL/reveal-js/plugin/notes/notes.js"></script>
  
  
  <script type="text/javascript" src="/presentation-2024-DSRT-batch-RL/reveal-js/plugin/notes/notes.js"></script>




<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }

  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };

  var revealHugoPlugins = { 
    plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealZoom ]
   };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams),
    camelize(revealHugoPlugins));
  Reveal.initialize(options);
</script>






  

  

  



    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>

<script type="text/javascript" id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>

<script>
  if (/.*?(\?|&)print-pdf/.test(window.location.toString())) {
      var ytVideos = document.getElementsByTagName("iframe")
      for (let i = 0; i < ytVideos.length; i++) {
          var videoFrame = ytVideos[i]
          var isYouTube = /^https?:\/\/(www.)youtube\.com\/.*/.test(videoFrame.src)
          if (isYouTube) {
              console.log(`Removing ${videoFrame.src}`)
              var parent = videoFrame.parentElement
              videoFrame.remove()
              var p = document.createElement('p')
              p.append(
                  document.createTextNode(
                      "There was an embedded video here, but it is disabled in the printed version of the slides."
                  )
              )
              p.append(document.createElement('br'))
              p.append(
                  document.createTextNode(
                      `Visit instead ${
                          videoFrame.src
                      } or ${
                          videoFrame.src.replace(
                              /(^https?:\/\/(www.)youtube\.com)\/(embed\/)(\w+).*/,
                              "https://www.youtube.com/watch?v=$4"
                          )
                      }`
                  )
              )
              parent.appendChild(p)
          }
      }
  }
</script>


    
  

</body></html>